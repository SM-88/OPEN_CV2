{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"/heavy_data/smkim/workspace/#MES/web-crawler\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/heavy_data/smkim/workspace/#MES/web-crawler'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0% [                                                ]    4792320 / 1008519338^C\n",
      "Traceback (most recent call last):\n",
      "  File \"kowiki.py\", line 58, in <module>\n",
      "    filename = wget.download(\"https://dumps.wikimedia.org/kowiki/latest/kowiki-latest-pages-meta-current.xml.bz2\", args.output)\n",
      "  File \"/heavy_data/smkim/.virtualenvs/smkim/lib/python3.6/site-packages/wget.py\", line 526, in download\n",
      "    (tmpfile, headers) = ulib.urlretrieve(binurl, tmpfile, callback)\n",
      "  File \"/usr/lib/python3.6/urllib/request.py\", line 277, in urlretrieve\n",
      "    block = fp.read(bs)\n",
      "  File \"/usr/lib/python3.6/http/client.py\", line 463, in read\n",
      "    n = self.readinto(b)\n",
      "  File \"/usr/lib/python3.6/http/client.py\", line 507, in readinto\n",
      "    n = self.fp.readinto(b)\n",
      "  File \"/usr/lib/python3.6/socket.py\", line 586, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "  File \"/usr/lib/python3.6/ssl.py\", line 1012, in recv_into\n",
      "    return self.read(nbytes, buffer)\n",
      "  File \"/usr/lib/python3.6/ssl.py\", line 874, in read\n",
      "    return self._sslobj.read(len, buffer)\n",
      "  File \"/usr/lib/python3.6/ssl.py\", line 631, in read\n",
      "    v = self._sslobj.read(len, buffer)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python kowiki.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/heavy_data/smkim/workspace/#MES/web-crawler'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv.field_size_limit(100000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv.field_size_limit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-39cbecb6f894>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSEPARATOR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"python\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# title 과 text를 중복 되므로 text만 저장 함\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\\n\\n\\n\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 구분자\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/smkim/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36miterrows\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1012\u001b[0m         \u001b[0mklass\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_constructor_sliced\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mklass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1015\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/smkim/lib/python3.6/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[1;32m    325\u001b[0m                     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msanitize_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_cast_failure\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSingleBlockManager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/smkim/lib/python3.6/site-packages/pandas/core/construction.py\u001b[0m in \u001b[0;36msanitize_array\u001b[0;34m(data, index, dtype, copy, raise_cast_failure)\u001b[0m\n\u001b[1;32m    498\u001b[0m             \u001b[0msubarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray_tuplesafe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 500\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mis_extension_array_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubarr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mis_extension_array_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    501\u001b[0m         \u001b[0;31m# This is to prevent mixed-type Series getting all casted to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m         \u001b[0;31m# NumPy string type, e.g. NaN --> '-1#IND'.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/smkim/lib/python3.6/site-packages/pandas/core/dtypes/common.py\u001b[0m in \u001b[0;36mis_extension_array_dtype\u001b[0;34m(arr_or_dtype)\u001b[0m\n\u001b[1;32m   1504\u001b[0m     \"\"\"\n\u001b[1;32m   1505\u001b[0m     \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr_or_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"dtype\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marr_or_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1506\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExtensionDtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mregistry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/smkim/lib/python3.6/site-packages/pandas/core/dtypes/base.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    425\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m             \u001b[0mdtype_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m                 \u001b[0mdtype_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0missubclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExtensionDtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "in_file = \"kowiki/kowiki_20211230.csv\"\n",
    "out_file = \"kowiki.txt\"\n",
    "SEPARATOR = u\"\\u241D\"\n",
    "df = pd.read_csv(in_file, sep=SEPARATOR, engine=\"python\")\n",
    "with open(out_file, \"w\") as f:\n",
    "    for index, row in df.iterrows():\n",
    "        f.write(row[\"text\"]) # title 과 text를 중복 되므로 text만 저장 함\n",
    "        f.write(\"\\n\\n\\n\\n\") # 구분자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "corpus = \"kowiki3.txt\"\n",
    "prefix = \"kowiki3\"\n",
    "vocab_size = 3000\n",
    "spm.SentencePieceTrainer.train(\n",
    "    f\"--input={corpus} --model_prefix={prefix} --vocab_size={vocab_size + 7}\" + \n",
    "    \" --model_type=bpe\" +\n",
    "    \" --max_sentence_length=999999\" + # 문장 최대 길이\n",
    "    \" --pad_id=0 --pad_piece=[PAD]\" + # pad (0)\n",
    "    \" --unk_id=1 --unk_piece=[UNK]\" + # unknown (1)\n",
    "    \" --bos_id=2 --bos_piece=[BOS]\" + # begin of sequence (2)\n",
    "    \" --eos_id=3 --eos_piece=[EOS]\" + # end of sequence (3)\n",
    "    \" --user_defined_symbols=[SEP],[CLS],[MASK]\") # 사용자 정의 토큰\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "겨울이 되어서 날씨가 무척 추워요.\n",
      "['▁', '겨', '울', '이', '▁', '되어', '서', '▁', '날', '씨', '가', '▁무', '척', '▁추', '워', '요', '.']\n",
      "[351, 739, 656, 352, 351, 105, 368, 351, 690, 815, 362, 115, 968, 172, 743, 511, 356]\n",
      "\n",
      "이번 성탄절은 화이트 크리스마스가 될까요?\n",
      "['▁이', '번', '▁성', '탄', '절', '은', '▁화', '이', '트', '▁크', '리', '스', '마', '스', '가', '▁', '될', '까', '요', '?']\n",
      "[7, 577, 86, 677, 646, 364, 210, 352, 462, 223, 371, 388, 422, 388, 362, 351, 786, 551, 511, 1199]\n",
      "\n",
      "겨울에 감기 조심하시고 행복한 연말 되세요.\n",
      "['▁', '겨', '울', '에', '▁감', '기', '▁조', '심', '하', '시', '고', '▁행', '복', '한', '▁연', '말', '▁되', '세', '요', '.']\n",
      "[351, 739, 656, 355, 286, 369, 53, 575, 358, 374, 366, 313, 600, 365, 71, 578, 169, 446, 511, 356]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "vocab_file = \"kowiki3.model\"\n",
    "vocab = spm.SentencePieceProcessor()\n",
    "vocab.load(vocab_file)\n",
    "\n",
    "lines = [\n",
    "  \"겨울이 되어서 날씨가 무척 추워요.\",\n",
    "  \"이번 성탄절은 화이트 크리스마스가 될까요?\",\n",
    "  \"겨울에 감기 조심하시고 행복한 연말 되세요.\"\n",
    "]\n",
    "for line in lines:\n",
    "  pieces = vocab.encode_as_pieces(line)\n",
    "  ids = vocab.encode_as_ids(line)\n",
    "  print(line)\n",
    "  print(pieces)\n",
    "  print(ids)\n",
    "  print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_enc_vocab': 3007, 'n_enc_seq': 256, 'n_seg_type': 2, 'n_layer': 6, 'd_hidn': 256, 'i_pad': 0, 'd_ff': 1024, 'n_head': 4, 'd_head': 64, 'dropout': 0.1, 'layer_norm_epsilon': 1e-12}\n"
     ]
    }
   ],
   "source": [
    "\"\"\" configuration json을 읽어들이는 class \"\"\"\n",
    "class Config(dict): \n",
    "    __getattr__ = dict.__getitem__\n",
    "    __setattr__ = dict.__setitem__\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, file):\n",
    "        with open(file, 'r') as f:\n",
    "            config = json.loads(f.read())\n",
    "            return Config(config)\n",
    "        \n",
    "config = Config({\n",
    "    \"n_enc_vocab\": len(vocab),\n",
    "    \"n_enc_seq\": 256,\n",
    "    \"n_seg_type\": 2,\n",
    "    \"n_layer\": 6,\n",
    "    \"d_hidn\": 256,\n",
    "    \"i_pad\": 0,\n",
    "    \"d_ff\": 1024,\n",
    "    \"n_head\": 4,\n",
    "    \"d_head\": 64,\n",
    "    \"dropout\": 0.1,\n",
    "    \"layer_norm_epsilon\": 1e-12\n",
    "})\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\"\"\" encoder layer \"\"\"\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.self_attn = MultiHeadAttention(self.config)\n",
    "        self.layer_norm1 = nn.LayerNorm(self.config.d_hidn, eps=self.config.layer_norm_epsilon)\n",
    "        self.pos_ffn = PoswiseFeedForwardNet(self.config)\n",
    "        self.layer_norm2 = nn.LayerNorm(self.config.d_hidn, eps=self.config.layer_norm_epsilon)\n",
    "    \n",
    "    def forward(self, inputs, attn_mask):\n",
    "        # (bs, n_enc_seq, d_hidn), (bs, n_head, n_enc_seq, n_enc_seq)\n",
    "        att_outputs, attn_prob = self.self_attn(inputs, inputs, inputs, attn_mask)\n",
    "        att_outputs = self.layer_norm1(inputs + att_outputs)\n",
    "        # (bs, n_enc_seq, d_hidn)\n",
    "        ffn_outputs = self.pos_ffn(att_outputs)\n",
    "        ffn_outputs = self.layer_norm2(ffn_outputs + att_outputs)\n",
    "        # (bs, n_enc_seq, d_hidn), (bs, n_head, n_enc_seq, n_enc_seq)\n",
    "        return ffn_outputs, attn_prob\n",
    "    \n",
    "\"\"\" encoder \"\"\"\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.enc_emb = nn.Embedding(self.config.n_enc_vocab, self.config.d_hidn) #Token embeddings\n",
    "        self.pos_emb = nn.Embedding(self.config.n_enc_seq + 1, self.config.d_hidn) #position Embeddings\n",
    "        self.seg_emb = nn.Embedding(self.config.n_seg_type, self.config.d_hidn) #segment Embeddings\n",
    "\n",
    "        self.layers = nn.ModuleList([EncoderLayer(self.config) for _ in range(self.config.n_layer)])\n",
    "    \n",
    "    def forward(self, inputs, segments): #Encoder input에 segment정보 추가 \n",
    "        positions = torch.arange(inputs.size(1), device=inputs.device, dtype=inputs.dtype).expand(inputs.size(0), inputs.size(1)).contiguous() + 1\n",
    "        pos_mask = inputs.eq(self.config.i_pad)\n",
    "        positions.masked_fill_(pos_mask, 0)\n",
    "\n",
    "        # (bs, n_enc_seq, d_hidn)\n",
    "        outputs = self.enc_emb(inputs) + self.pos_emb(positions)  + self.seg_emb(segments) #token, position, segment 3가지 embedding +\n",
    "\n",
    "        # (bs, n_enc_seq, n_enc_seq)\n",
    "        attn_mask = get_attn_pad_mask(inputs, inputs, self.config.i_pad)\n",
    "\n",
    "        attn_probs = []\n",
    "        for layer in self.layers:\n",
    "            # (bs, n_enc_seq, d_hidn), (bs, n_head, n_enc_seq, n_enc_seq)\n",
    "            outputs, attn_prob = layer(outputs, attn_mask)\n",
    "            attn_probs.append(attn_prob)\n",
    "        # (bs, n_enc_seq, d_hidn), [(bs, n_head, n_enc_seq, n_enc_seq)]\n",
    "        return outputs, attn_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" bert \"\"\"\n",
    "class BERT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.encoder = Encoder(self.config)\n",
    "\n",
    "        self.linear = nn.Linear(config.d_hidn, config.d_hidn)\n",
    "        self.activation = torch.tanh\n",
    "    \n",
    "    def forward(self, inputs, segments):\n",
    "        # (bs, n_seq, d_hidn), [(bs, n_head, n_enc_seq, n_enc_seq)]\n",
    "        outputs, self_attn_probs = self.encoder(inputs, segments) #Transformer Encoder \n",
    "        # (bs, d_hidn)\n",
    "        outputs_cls = outputs[:, 0].contiguous()\n",
    "        outputs_cls = self.linear(outputs_cls) # outpdits_cls에 Linear 및 tanh 실행\n",
    "        outputs_cls = self.activation(outputs_cls)\n",
    "        # (bs, n_enc_seq, n_enc_vocab), (bs, d_hidn), [(bs, n_head, n_enc_seq, n_enc_seq)]\n",
    "        return outputs, outputs_cls, self_attn_probs\n",
    "    \n",
    "    def save(self, epoch, loss, path):  #Pretrained save\n",
    "        torch.save({\n",
    "            \"epoch\": epoch,\n",
    "            \"loss\": loss,\n",
    "            \"state_dict\": self.state_dict()\n",
    "        }, path)\n",
    "    \n",
    "    def load(self, path):  #Pretrained load\n",
    "        save = torch.load(path)\n",
    "        self.load_state_dict(save[\"state_dict\"])\n",
    "        return save[\"epoch\"], save[\"loss\"]\n",
    "    \n",
    "\"\"\" BERT pretrain \"\"\"\n",
    "class BERTPretrain(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.bert = BERT(self.config)\n",
    "        \n",
    "        # classfier - #BERT의 결과를 입력으로 NSP를 예측하기위한 projection_cls를 선언\n",
    "        self.projection_cls = nn.Linear(self.config.d_hidn, 2, bias=False) \n",
    "        \n",
    "        # lm - #BERT의 결과를 입력으로 MLM을 예측하기위한 projection_lm을 선언\n",
    "        self.projection_lm = nn.Linear(self.config.d_hidn, self.config.n_enc_vocab, bias=False)\n",
    "        \n",
    "        #projection_lm은 Encoder의 Embedding과 weight를 share \n",
    "        self.projection_lm.weight = self.bert.encoder.enc_emb.weight\n",
    "    \n",
    "    def forward(self, inputs, segments):\n",
    "        # (bs, n_enc_seq, d_hidn), (bs, d_hidn), [(bs, n_head, n_enc_seq, n_enc_seq)]\n",
    "        \n",
    "        # inputs, segments를 입력으로 BERT를 실행\n",
    "        outputs, outputs_cls, attn_probs = self.bert(inputs, segments)\n",
    "        \n",
    "        # (bs, 2) - # outputs_cls를 입력으로 projection_cls를 실행하여 NSP를 예측\n",
    "        logits_cls = self.projection_cls(outputs_cls)\n",
    "        \n",
    "        # (bs, n_enc_seq, n_enc_vocab) - #outputs를 입력으로 projection_lm을 실행하여 MLM을 예측\n",
    "        logits_lm = self.projection_lm(outputs)\n",
    "        \n",
    "        # (bs, n_enc_vocab), (bs, n_enc_seq, n_enc_vocab), [(bs, n_head, n_enc_seq, n_enc_seq)]\n",
    "        return logits_cls, logits_lm, attn_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\"\"\" 마스크 생성 함수 \"\"\"\n",
    "def create_pretrain_mask(tokens, mask_cnt, vocab_list): #token을 단어별로 index 배열 행태로 저장 (4~ 10)\n",
    "    cand_idx = []\n",
    "    for (i, token) in enumerate(tokens):\n",
    "        if token == \"[CLS]\" or token == \"[SEP]\":\n",
    "            continue\n",
    "        if 0 < len(cand_idx) and not token.startswith(u\"\\u2581\"):\n",
    "            cand_idx[-1].append(i)\n",
    "        else:\n",
    "            cand_idx.append([i]) # index를 for loop를 돌며 아래내용(4 ~ 9)을 실행 \n",
    "    random.shuffle(cand_idx) #Random 선택을 위해 단어의 index shuffle\n",
    "\n",
    "    mask_lms = []\n",
    "    for index_set in cand_idx: #mask_lms의 개수가 mask_cnt를 넘지 않도록 (15~ 18)\n",
    "        if len(mask_lms) >= mask_cnt:#mask_cnt는 전체 token개수의 15%에 해당하는 개수\n",
    "            break\n",
    "        if len(mask_lms) + len(index_set) > mask_cnt:\n",
    "            continue\n",
    "        for index in index_set:\n",
    "            masked_token = None\n",
    "            if random.random() < 0.8: # 80% replace with [MASK] #index에 대해 80% 확률로 [MASK]를 취합니다(21~22)\n",
    "                masked_token = \"[MASK]\"\n",
    "            else:\n",
    "                if random.random() < 0.5: # 10% keep original #index에 대해 10% 확률로 현재 값을 유지(24~25)\n",
    "                    masked_token = tokens[index]\n",
    "                else: # 10% random word                # index에 대해 10% 확률로 vocab_list에서 임의의 값을 선택(26~ 27)\n",
    "                    masked_token = random.choice(vocab_list)\n",
    "            mask_lms.append({\"index\": index, \"label\": tokens[index]}) #mask된 index의 값과 정답 label을 mask_lms에 저장(28)\n",
    "            tokens[index] = masked_token #token index의 값을 mask (29)\n",
    "    mask_lms = sorted(mask_lms, key=lambda x: x[\"index\"]) #Random하게 mask된 값을 index순으로 정렬 (30)\n",
    "    mask_idx = [p[\"index\"] for p in mask_lms]  # 정렬된 값을 이용해 mask_index, mask_label을 생성 (31~32)\n",
    "    mask_label = [p[\"label\"] for p in mask_lms]\n",
    "\n",
    "    return tokens, mask_idx, mask_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" 쵀대 길이 초과하는 토큰 자르기 \"\"\"\n",
    "def trim_tokens(tokens_a, tokens_b, max_seq):\n",
    "    while True:\n",
    "        total_length = len(tokens_a) + len(tokens_b)\n",
    "        if total_length <= max_seq:\n",
    "            break\n",
    "\n",
    "        if len(tokens_a) > len(tokens_b): #token A의 길이가 길 경우 앞에서부터 토큰을 제거(8~9)\n",
    "            del tokens_a[0]\n",
    "        else:                             #token B의 길이가 길 경우 앞에서부터 토큰을 제거(10~11)\n",
    "            tokens_b.pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" doc별 pretrain 데이터 생성 \"\"\" , # 단락별 pretrain 데이터 생성 함수 \n",
    "from random import randrange\n",
    "def create_pretrain_instances(docs, doc_idx, doc, n_seq, mask_prob, vocab_list):\n",
    "    # for [CLS], [SEP], [SEP]\n",
    "    max_seq = n_seq - 3  #tgt_seq는 n_seq에서 3개를 뺀 값\n",
    "    tgt_seq = max_seq\n",
    "    \n",
    "    instances = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "    \n",
    "    for i in range(len(doc)): #단락을 줄 단위로 for loop를 돌며 아래내용(3 ~ 12)을 실행\n",
    "        \n",
    "        current_chunk.append(doc[i]) # line  #current_chunk에 line을 추가, current_length에 라인의 token 수 더하기(11,12)\n",
    "        current_length += len(doc[i])\n",
    "        if i == len(doc) - 1 or current_length >= tgt_seq: #마지막 줄 이거나 current_length가 tgt_seq를 넘을 경우 학습데이터 만듦\n",
    "            if 0 < len(current_chunk):\n",
    "                a_end = 1                                 #current_chunk에서 Random하게 길이를 선택해서 tokens_a를 만듬(15~20)\n",
    "                if 1 < len(current_chunk):\n",
    "                    a_end = randrange(1, len(current_chunk))\n",
    "                tokens_a = []\n",
    "                for j in range(a_end):\n",
    "                    \n",
    "                    tokens_a.extend(current_chunk[j])\n",
    "                \n",
    "                tokens_b = []\n",
    "                if len(current_chunk) == 1 or random.random() < 0.5:  #50%의 확률로 다른 단락에서 tokens_b를 만듦(23~33)\n",
    "                    is_next = 0                                 #is_next의 값은 False(0)\n",
    "                    tokens_b_len = tgt_seq - len(tokens_a)\n",
    "                    random_doc_idx = doc_idx\n",
    "                    while doc_idx == random_doc_idx:\n",
    "                       # print(\"doc_idx: \", doc_idx)\n",
    "                        #print(\"random_doc_idx_before: \", random_doc_idx)\n",
    "                        random_doc_idx = randrange(0, len(docs))\n",
    "                        #print(\"random_doc_idx_after: \", random_doc_idx)\n",
    "                    random_doc = docs[random_doc_idx]\n",
    "\n",
    "                    random_start = randrange(0, len(random_doc))\n",
    "                    for j in range(random_start, len(random_doc)):\n",
    "                        tokens_b.extend(random_doc[j])\n",
    "                else:                                          #50%의 확률로 current_chunk에서 tokens_a 이후부터 tokens_b를 만듬(34~37)\n",
    "                    is_next = 1                                #is_next의 값은 True(1)\n",
    "                    for j in range(a_end, len(current_chunk)):\n",
    "                        \n",
    "                        tokens_b.extend(current_chunk[j])\n",
    "\n",
    "                trim_tokens(tokens_a, tokens_b, max_seq)         #위에서 정의한 trim_tokens 함수를 실행하여 token 크기를 줄임(39)\n",
    "                assert 0 < len(tokens_a)\n",
    "                assert 0 < len(tokens_b)\n",
    "\n",
    "                tokens = [\"[CLS]\"] + tokens_a + [\"[SEP]\"] + tokens_b + [\"[SEP]\"] #‘[CLS]’ + tokens_a + ‘[SEP]’ + tokens_b + ‘[SEP]’ 형태로 데이터를 생성(43)\n",
    "                segment = [0] * (len(tokens_a) + 2) + [1] * (len(tokens_b) + 1) #segment를 생성 #윗줄에서 '[CLS]’ + tokens_a + ‘[SEP]’는 0, tokens_b + ‘[SEP]’는 1\n",
    "                \n",
    "                tokens, mask_idx, mask_label = create_pretrain_mask(tokens, int((len(tokens) - 3) * mask_prob), vocab_list) #정의한 create_pretrain_mask 함수를 실행하여 Mask \n",
    "                                                                                                                           #Mask Token 개수는 전체 Token 수에 0.15(15%)를 곱\n",
    "                instance = {               #위 결과를 가지고 데이터를 생성 (48~ 55)\n",
    "                    \"tokens\": tokens,\n",
    "                    \"segment\": segment,\n",
    "                    \"is_next\": is_next,\n",
    "                    \"mask_idx\": mask_idx,\n",
    "                    \"mask_label\": mask_label\n",
    "                }\n",
    "                instances.append(instance)\n",
    "\n",
    "            current_chunk = []\n",
    "            current_length = 0\n",
    "    return instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" pretrain 데이터 생성 \"\"\"  #pretrain 데이터 생성함수 \n",
    "import tqdm\n",
    "from random import randrange\n",
    "import random\n",
    "import json\n",
    "from tqdm import tqdm_notebook\n",
    "# ex ) for i in tqdm_notebook(range(10000)) :  ;  print(i)\n",
    "\n",
    "def make_pretrain_data(vocab, in_file, out_file, count, n_seq, mask_prob):\n",
    "    vocab_list = []\n",
    "    for id in range(vocab.get_piece_size()):\n",
    "        if not vocab.is_unknown(id):\n",
    "            vocab_list.append(vocab.id_to_piece(id))\n",
    "\n",
    "    line_cnt = 0\n",
    "    with open(in_file, \"r\") as in_f:\n",
    "        for line in in_f:\n",
    "            line_cnt += 1\n",
    "    \n",
    "    docs = []\n",
    "    with open(in_file, \"r\") as f:\n",
    "        doc = []\n",
    "        with tqdm_notebook(total=line_cnt, desc=f\"Loading\") as pbar:\n",
    "            for i, line in enumerate(f):\n",
    "                line = line.strip()\n",
    "                if line == \"\":\n",
    "                    if 0 < len(doc):\n",
    "                        docs.append(doc)\n",
    "                        doc = []\n",
    "                else:\n",
    "                    pieces = vocab.encode_as_pieces(line)\n",
    "                    if 0 < len(pieces):\n",
    "                        doc.append(pieces)\n",
    "                pbar.update(1)\n",
    "        if doc:\n",
    "            docs.append(doc)\n",
    "\n",
    "    for index in range(count):\n",
    "        output = out_file.format(index)\n",
    "        if os.path.isfile(output): continue\n",
    "\n",
    "        with open(output, \"w\") as out_f:\n",
    "            with tqdm_notebook(total=len(docs), desc=f\"Making\") as pbar:\n",
    "                for i, doc in enumerate(docs):\n",
    "                    instances = create_pretrain_instances(docs, i, doc, n_seq, mask_prob, vocab_list)\n",
    "                    for instance in instances:\n",
    "                        out_f.write(json.dumps(instance))\n",
    "                        out_f.write(\"\\n\")\n",
    "                    pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/heavy_data/smkim/.virtualenvs/smkim/lib/python3.6/site-packages/ipykernel_launcher.py:23: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a26993aa896043fdbc72a6817f5fa381",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Loading'), FloatProgress(value=0.0, max=13289.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" pretrain 데이터 생성 실행\"\"\"\n",
    "\n",
    "# 말뭉치 개수(count)는 10로 합니다.\n",
    "# sequence 길이(n_seq)는 256으로 합니다.\n",
    "# Mask 확률(mask_prob)는 15%로 합니다.\n",
    "\n",
    "in_file = \"kowiki3.txt\"\n",
    "out_file = \"kowiki3_bert_{}.json\"\n",
    "count = 10\n",
    "n_seq = 256\n",
    "mask_prob = 0.15\n",
    "\n",
    "make_pretrain_data(vocab, in_file, out_file, count, n_seq, mask_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "class PretrainDataSet(torch.utils.data.Dataset):\n",
    "    def __init__(self, vocab, infile):\n",
    "        self.vocab = vocab\n",
    "        self.labels_cls = []\n",
    "        self.labels_lm = []\n",
    "        self.sentences = []\n",
    "        self.segments = []\n",
    "\n",
    "        line_cnt = 0\n",
    "        with open(infile, \"r\") as f:\n",
    "            for line in f:\n",
    "                line_cnt += 1\n",
    "\n",
    "        with open(infile, \"r\") as f:\n",
    "            for i, line in enumerate(tqdm_notebook(f, total=line_cnt, desc=f\"Loading {infile}\", unit=\" lines\")):\n",
    "                instance = json.loads(line)\n",
    "                self.labels_cls.append(instance[\"is_next\"])\n",
    "                sentences = [vocab.piece_to_id(p) for p in instance[\"tokens\"]]\n",
    "                self.sentences.append(sentences)\n",
    "                self.segments.append(instance[\"segment\"])\n",
    "                mask_idx = np.array(instance[\"mask_idx\"], dtype=np.int)\n",
    "                mask_label = np.array([vocab.piece_to_id(p) for p in instance[\"mask_label\"]], dtype=np.int)\n",
    "                label_lm = np.full(len(sentences), dtype=np.int, fill_value=-1)\n",
    "                label_lm[mask_idx] = mask_label\n",
    "                self.labels_lm.append(label_lm)\n",
    "    \n",
    "    def __len__(self):\n",
    "        assert len(self.labels_cls) == len(self.labels_lm)\n",
    "        assert len(self.labels_cls) == len(self.sentences)\n",
    "        assert len(self.labels_cls) == len(self.segments)\n",
    "        return len(self.labels_cls)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        return (torch.tensor(self.labels_cls[item]),\n",
    "                torch.tensor(self.labels_lm[item]),\n",
    "                torch.tensor(self.sentences[item]),\n",
    "                torch.tensor(self.segments[item]))\n",
    "    \n",
    "def pretrin_collate_fn(inputs):\n",
    "    labels_cls, labels_lm, inputs, segments = list(zip(*inputs))\n",
    "\n",
    "    labels_lm = torch.nn.utils.rnn.pad_sequence(labels_lm, batch_first=True, padding_value=-1)\n",
    "    inputs = torch.nn.utils.rnn.pad_sequence(inputs, batch_first=True, padding_value=0)\n",
    "    segments = torch.nn.utils.rnn.pad_sequence(segments, batch_first=True, padding_value=0)\n",
    "\n",
    "    batch = [\n",
    "        torch.stack(labels_cls, dim=0),\n",
    "        labels_lm,\n",
    "        inputs,\n",
    "        segments\n",
    "    ]\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/heavy_data/smkim/.virtualenvs/smkim/lib/python3.6/site-packages/ipykernel_launcher.py:16: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b19cd642424245db940d20460322620c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Loading kowiki3_bert_1.json'), FloatProgress(value=0.0, max=4130.0), HTML(value='')…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" pretrain 데이터 로더 \"\"\"\n",
    "import numpy as np\n",
    "batch_size = 128\n",
    "dataset = PretrainDataSet(vocab, \"kowiki3_bert_1.json\")\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=pretrin_collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/heavy_data/smkim/.virtualenvs/smkim/lib/python3.6/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 10010). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 모델 epoch 학습 \"\"\"\n",
    "def train_epoch(config, epoch, model, criterion_lm, criterion_cls, optimizer, train_loader):\n",
    "    losses = []\n",
    "    model.train()\n",
    "\n",
    "    with tqdm_notebook(total=len(train_loader), desc=f\"Train({epoch})\") as pbar:\n",
    "        for i, value in enumerate(train_loader):\n",
    "            labels_cls, labels_lm, inputs, segments = map(lambda v: v.to(config.device), value)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs, segments)\n",
    "            logits_cls, logits_lm = outputs[0], outputs[1]\n",
    "\n",
    "            loss_cls = criterion_cls(logits_cls, labels_cls)\n",
    "            loss_lm = criterion_lm(logits_lm.view(-1, logits_lm.size(2)), labels_lm.view(-1))\n",
    "            loss = loss_cls + loss_lm\n",
    "\n",
    "            loss_val = loss_lm.item()\n",
    "            losses.append(loss_val)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            pbar.update(1)\n",
    "            pbar.set_postfix_str(f\"Loss: {loss_val:.3f} ({np.mean(losses):.3f})\")\n",
    "    return np.mean(losses)\n",
    "\n",
    "config.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#print(config)\n",
    "\n",
    "learning_rate = 5e-5\n",
    "n_epoch = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\"\"\" multi head attention \"\"\"\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.W_Q = nn.Linear(self.config.d_hidn, self.config.n_head * self.config.d_head)\n",
    "        self.W_K = nn.Linear(self.config.d_hidn, self.config.n_head * self.config.d_head)\n",
    "        self.W_V = nn.Linear(self.config.d_hidn, self.config.n_head * self.config.d_head)\n",
    "        self.scaled_dot_attn = ScaledDotProductAttention(self.config)\n",
    "        self.linear = nn.Linear(self.config.n_head * self.config.d_head, self.config.d_hidn)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "    \n",
    "    def forward(self, Q, K, V, attn_mask):\n",
    "        batch_size = Q.size(0)\n",
    "        # (bs, n_head, n_q_seq, d_head)\n",
    "        q_s = self.W_Q(Q).view(batch_size, -1, self.config.n_head, self.config.d_head).transpose(1,2)\n",
    "        # (bs, n_head, n_k_seq, d_head)\n",
    "        k_s = self.W_K(K).view(batch_size, -1, self.config.n_head, self.config.d_head).transpose(1,2)\n",
    "        # (bs, n_head, n_v_seq, d_head)\n",
    "        v_s = self.W_V(V).view(batch_size, -1, self.config.n_head, self.config.d_head).transpose(1,2)\n",
    "\n",
    "        # (bs, n_head, n_q_seq, n_k_seq)\n",
    "        attn_mask = attn_mask.unsqueeze(1).repeat(1, self.config.n_head, 1, 1)\n",
    "\n",
    "        # (bs, n_head, n_q_seq, d_head), (bs, n_head, n_q_seq, n_k_seq)\n",
    "        context, attn_prob = self.scaled_dot_attn(q_s, k_s, v_s, attn_mask)\n",
    "        # (bs, n_head, n_q_seq, h_head * d_head)\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.config.n_head * self.config.d_head)\n",
    "        # (bs, n_head, n_q_seq, e_embd)\n",
    "        output = self.linear(context)\n",
    "        output = self.dropout(output)\n",
    "        # (bs, n_q_seq, d_hidn), (bs, n_head, n_q_seq, n_k_seq)\n",
    "        return output, attn_prob\n",
    "    \n",
    "\"\"\" scale dot product attention \"\"\"\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        self.scale = 1 / (self.config.d_head ** 0.5)\n",
    "    \n",
    "    def forward(self, Q, K, V, attn_mask):\n",
    "        # (bs, n_head, n_q_seq, n_k_seq)\n",
    "        scores = torch.matmul(Q, K.transpose(-1, -2))\n",
    "        scores = scores.mul_(self.scale)\n",
    "        scores.masked_fill_(attn_mask, -1e9)\n",
    "        # (bs, n_head, n_q_seq, n_k_seq)\n",
    "        attn_prob = nn.Softmax(dim=-1)(scores)\n",
    "        attn_prob = self.dropout(attn_prob)\n",
    "        # (bs, n_head, n_q_seq, d_v)\n",
    "        context = torch.matmul(attn_prob, V)\n",
    "        # (bs, n_head, n_q_seq, d_v), (bs, n_head, n_q_seq, n_v_seq)\n",
    "        return context, attn_prob\n",
    "    \n",
    "\"\"\" feed forward \"\"\"\n",
    "class PoswiseFeedForwardNet(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.conv1 = nn.Conv1d(in_channels=self.config.d_hidn, out_channels=self.config.d_ff, kernel_size=1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=self.config.d_ff, out_channels=self.config.d_hidn, kernel_size=1)\n",
    "        self.active = F.gelu\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # (bs, d_ff, n_seq)\n",
    "        output = self.conv1(inputs.transpose(1, 2))\n",
    "        output = self.active(output)\n",
    "        # (bs, n_seq, d_hidn)\n",
    "        output = self.conv2(output).transpose(1, 2)\n",
    "        output = self.dropout(output)\n",
    "        # (bs, n_seq, d_hidn)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" attention pad mask \"\"\"\n",
    "def get_attn_pad_mask(seq_q, seq_k, i_pad):\n",
    "    batch_size, len_q = seq_q.size()\n",
    "    batch_size, len_k = seq_k.size()\n",
    "    pad_attn_mask = seq_k.data.eq(i_pad)\n",
    "    pad_attn_mask= pad_attn_mask.unsqueeze(1).expand(batch_size, len_q, len_k)\n",
    "    return pad_attn_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load pretrain from: save_bert_pretrain.pth, epoch=19, loss=12.016347971829502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/heavy_data/smkim/.virtualenvs/smkim/lib/python3.6/site-packages/ipykernel_launcher.py:16: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11156d1dd539468d969053a57ff6220b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Loading kowiki3_bert_1.json'), FloatProgress(value=0.0, max=4130.0), HTML(value='')…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2\n",
      "train_loader: <torch.utils.data.dataloader.DataLoader object at 0x7f6e65242198>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/heavy_data/smkim/.virtualenvs/smkim/lib/python3.6/site-packages/ipykernel_launcher.py:6: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee19c65257304434bf9540fbae675ad4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Train(21)'), FloatProgress(value=0.0, max=33.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d8c26c78ca4491b9f2479f5cf8ba1c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Loading kowiki3_bert_2.json'), FloatProgress(value=0.0, max=4130.0), HTML(value='')…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2\n",
      "train_loader: <torch.utils.data.dataloader.DataLoader object at 0x7f6e6469cb70>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a3e8b0a731f4cceb903c90febad0359",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Train(22)'), FloatProgress(value=0.0, max=33.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f97bf3650a5403a8e07334fca19d18d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Loading kowiki3_bert_3.json'), FloatProgress(value=0.0, max=2436.0), HTML(value='')…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2\n",
      "train_loader: <torch.utils.data.dataloader.DataLoader object at 0x7f6e651cef98>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9497b5a226c347208e2e829b7ef138c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Train(23)'), FloatProgress(value=0.0, max=20.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb46e34e45d846528e0a63c5e0ba517e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Loading kowiki3_bert_4.json'), FloatProgress(value=0.0, max=4130.0), HTML(value='')…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2\n",
      "train_loader: <torch.utils.data.dataloader.DataLoader object at 0x7f6e6522f2b0>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "251de342cd584a46b1d942f089a2c21d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Train(24)'), FloatProgress(value=0.0, max=33.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcb83b77b4334e9aa9da4a4bfcee862e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Loading kowiki3_bert_5.json'), FloatProgress(value=0.0, max=4130.0), HTML(value='')…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2\n",
      "train_loader: <torch.utils.data.dataloader.DataLoader object at 0x7f6e65237a20>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a77646d769564d4ca3d09e427dbf1b8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Train(25)'), FloatProgress(value=0.0, max=33.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d941e0eecc849b18f596f2fdf6dfff3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Loading kowiki3_bert_6.json'), FloatProgress(value=0.0, max=4130.0), HTML(value='')…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2\n",
      "train_loader: <torch.utils.data.dataloader.DataLoader object at 0x7f6e65278588>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bee8747229b74102b1627fbfd1091a3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Train(26)'), FloatProgress(value=0.0, max=33.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec06b78c00364db79aaa40b860e5535f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Loading kowiki3_bert_7.json'), FloatProgress(value=0.0, max=4130.0), HTML(value='')…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2\n",
      "train_loader: <torch.utils.data.dataloader.DataLoader object at 0x7f6e65237080>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f51d81a377948209f082e36ebd7b5b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Train(27)'), FloatProgress(value=0.0, max=33.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce7b64d3a4d745acb29f78469380290e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Loading kowiki3_bert_8.json'), FloatProgress(value=0.0, max=4130.0), HTML(value='')…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2\n",
      "train_loader: <torch.utils.data.dataloader.DataLoader object at 0x7f6e652c8cc0>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33252efff92c44e68e4d459b9ad8fa79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Train(28)'), FloatProgress(value=0.0, max=33.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c138129aa8644279a8bef8ea8b9c45d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Loading kowiki3_bert_9.json'), FloatProgress(value=0.0, max=4130.0), HTML(value='')…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2\n",
      "train_loader: <torch.utils.data.dataloader.DataLoader object at 0x7f6e65237a20>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72becbd5f795491fbd6f531b323d5083",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Train(29)'), FloatProgress(value=0.0, max=33.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "724d51f9821e41e39e766ed7975d5d09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Loading kowiki3_bert_0.json'), FloatProgress(value=1.0, bar_style='info', layout=La…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "num_samples should be a positive integer value, but got num_samples=0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-9f1750dec908>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m#del train_loader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPretrainDataSet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"kowiki3_bert_{epoch % count}.json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mtrain_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpretrin_collate_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train_loader:'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/smkim/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers)\u001b[0m\n\u001b[1;32m    264\u001b[0m                     \u001b[0;31m# Cannot statically verify that dataset is Sized\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m                     \u001b[0;31m# Somewhat related: see NOTE [ Lack of Default `__len__` in Python Abstract Base Classes ]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m                     \u001b[0msampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m                     \u001b[0msampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequentialSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/smkim/lib/python3.6/site-packages/torch/utils/data/sampler.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data_source, replacement, num_samples, generator)\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_samples\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m             raise ValueError(\"num_samples should be a positive integer \"\n\u001b[0;32m--> 104\u001b[0;31m                              \"value, but got num_samples={}\".format(self.num_samples))\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: num_samples should be a positive integer value, but got num_samples=0"
     ]
    }
   ],
   "source": [
    "\n",
    "model = BERTPretrain(config)\n",
    "\n",
    "save_pretrain = \"save_bert_pretrain.pth\"\n",
    "best_epoch, best_loss = 0, 0\n",
    "if os.path.isfile(save_pretrain):\n",
    "    best_epoch, best_loss = model.bert.load(save_pretrain)\n",
    "    print(f\"load pretrain from: {save_pretrain}, epoch={best_epoch}, loss={best_loss}\")\n",
    "    best_epoch += 1\n",
    "\n",
    "model.to(config.device)\n",
    "\n",
    "criterion_lm = torch.nn.CrossEntropyLoss(ignore_index=-1, reduction='mean')\n",
    "criterion_cls = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "losses = []\n",
    "offset = best_epoch\n",
    "for step in range(1,n_epoch):\n",
    "    epoch = step + offset\n",
    "    if 0 <= step:\n",
    "        #del train_loader\n",
    "        dataset = PretrainDataSet(vocab, f\"kowiki3_bert_{epoch % count}.json\")\n",
    "        train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=pretrin_collate_fn)\n",
    "        print(2)\n",
    "        print('train_loader:',train_loader)\n",
    "    loss = train_epoch(config, epoch, model, criterion_lm, criterion_cls, optimizer, train_loader)\n",
    "    losses.append(loss)\n",
    "    model.bert.save(epoch, loss, save_pretrain)\n",
    "    \n",
    "    \n",
    "# data\n",
    "data = {\n",
    "    \"loss\": losses\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "display(df)\n",
    "\n",
    "# graph\n",
    "plt.figure(figsize=[8, 4])\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Depth')\n",
    "plt.xlim((0, n_epoch - 1))\n",
    "plt.ylabel('Position')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "smkim",
   "language": "python",
   "name": "smkim"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
